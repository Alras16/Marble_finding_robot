\documentclass[../Head/Main.tex]{subfiles}
\begin{document}
\subsection{Q-learning}
In order to effectively search the environment and collect marbles, a good search strategy must be found. This can be done by utilising reinforcement learning. By using reinforcement learning, the robot can learn from its experience and obtain a good strategy for navigating the environment.\par
By using a Temporal-Difference learning strategy the optimal action-value function can be estimated by every move taken, unlike a Monte Carlo strategy where an episode terminates before any learning is obtained. In some cases with long episodes the Monte Carlo strategy is considered too slow.\\
Generally there are two categories of Temporal-Difference learning; on-policy and off-policy methods. One of the advantages of an off-policy over an on-policy method are that the action-value function can be estimated independent from the policy being used. The policy only influences which state-action pairs that are visited and updated.\par 
Based on this, Q-learning is chosen. The Q-learning method builds on the following update function for updating the action-value function (Q-values).
\begin{equation}\label{eq:q_update_func}
Q\left(S_t,A_t\right) \leftarrow Q\left(S_t,A_t\right) + \alpha\left[R_{t+1}+\gamma\max_a Q\left(S_{t+1},a\right)-Q\left(S_t,A_t\right)\right]
\end{equation}
The update function for Q-learning consists of the old value for a given state-action combination plus a scaled difference between the old value, the immediate reward and the maximal value for the next state. The learning rate are denoted $\alpha$ and ranging from 0-1 preferable closer to 0, in order not to base the policy on this action only. $\gamma$ denotes the discount factor and is also ranging from 0-1, preferable closer to 1, to ensure that future actions matter.\par
In the box below, the algorithm for Q-learning can be seen.
\subfile{../Pseudo/Q-learning}
\subsubsection{Definition of states}
In order to perform Q-learning a definition of states must be made. These states must have the Markov property meaning that the probability of a marble being in a room must not depend on whether other rooms have been visited or not.\par 
In order to achieve this, it has been chosen to implement a vector of boolean values, one element for each room. This will be used to store which rooms have been visited.\\
By doing this, all possible combinations will be possible and the reward of entering a room will not depend on the other rooms, only the room itself.\par 
It has also been chosen to implement the state with an integer storing the room number and a boolean for storing whether the state is terminal or not. The definition of the state can be seen below.
\begin{Indentation}
	\item \texttt{qState} \vspace{-2pt}
	\begin{Indentation}
		\item \texttt{int roomNumber} \vspace{-2pt}
		\item \texttt{std::vector<bool> roomsVisited} \vspace{-2pt}
		\item \texttt{bool isTerminal}
	\end{Indentation}
\end{Indentation}
Due to the definition of the states, the number of states would grow rapidly with increasingly higher number of rooms. The nature of the state definition gives the possibility to divide the state space into smaller matrices, and by adding a bit of logic the mapping between the matrices could be obtained.\\
By defining a base state as the room number and sub-states as whether specific rooms have been visited or not, the state space could be reduced to a matrix with the size $(rooms + 1)\times(rooms + 1)$ and depth of $2^{room}-1$. The last combination would not be relevant because all rooms would have been visited. All states in this matrix would be a terminal state, meaning it would not make sense to move from there and therefore not make sense to update the Q-value of those states.\\
This principle can be seen on figure \ref{fig:3D-matrix}, where the numbers over the matrices describes the vector in the states.  
\begin{figure}[H]
	\centering
	\subfile{../Figures/3D-Q-matrix}
	\caption{Illustration of how the matrices are layered}
	\label{fig:3D-matrix}
\end{figure}


%\begin{Pseudo}{$\epsilon$-greedy policy}{}
	%\begin{Indentation}
		%\item if random number < $\epsilon$ \vspace{-2pt}
			%\begin{Indentation}
				%\item return random action \vspace{-2pt}
			%\end{Indentation}
		%\item else \vspace{-2pt}
		%\begin{Indentation}
			%\item return policy action
		%\end{Indentation}
	%\end{Indentation}
%\end{Pseudo}


%\begin{Pseudo}{getNextAction()}{}
	%\begin{Indentation}
		%\item loop all actions for a given state \vspace{-2pt}
		%\begin{Indentation}
			%\item if Q-value is higher than maxValue
		%\end{Indentation}
	%\end{Indentation}
%\end{Pseudo}
\end{document}