\documentclass[../Head/Main.tex]{subfiles}
\begin{document}
\subsection{The impact of $\alpha$ on Q-learning performance}
\label{subsec:test_alpha}
The purpose of this test is to show how different values of $\alpha$ influences the performance of Q-learning.
\subsubsection*{Description of test}
This test was done by performing 100 trials of each selected value of $\alpha$ for a range of episodes. The test was conducted using the world 5-room world seen on figure (\ref{fig:5_room_world_alpha}).\\
The distance punishments are bases on those found in the test in appendix (\ref{subsec:est_path_length}). The distance punishments was scaled with a factor of 1.2 to make the distance a bigger factor in final path.\\
The probabilities used for each room was found in the test in appendix (\ref{subsec:probability_test}). These probabilities was divided by the maximal value and scaled by a factor of 20. This was done to ensure that the total reward for entering a room the first time would be positive.\par 
The initial state for all tests was set to room 3 in order to ensure greatest number of possible paths.

\begin{figure}[H]
	\centering
	\subfile{../Figures/Map_5_rooms}	
	\caption{Illustration of "5-room world"}
	\label{fig:5_room_world_alpha}
\end{figure}

\subsubsection*{Test parameters}
In the following tables, the parameters for the test can be seen. It have been chosen to make the test on values of $\alpha$ between 0.01 and 0.5.\\
\begin{minipage}[c]{0.35\textwidth}
	\begin{tabular}{l r}
	- World used                   & 5-room world\\
	- Initial room                 & room 3\\	
	- Probabilities based on       & 50 tests\\	
	- Number of tests              & 100\\
	- Scaling factor distance      & 1.2\\
	- Scaling factor reward        & 20\\
	- Randomness factor $\epsilon$ & 0.05\\
	- Discount factor $\gamma$     & 0.9\\
	\end{tabular}
\end{minipage}	
\hfill
\begin{minipage}[c]{0.2\textwidth}
	\begin{table}[H]
		\centering
		\begin{tabular}{r r}
		\hline
		\multicolumn{2}{l}{\textbf{Tested values of $\alpha$}}\\ 			\hline
		0.01   & 0.15\\
		0.025  & 0.2\\
		0.05   & 0.3\\
		0.075  & 0.4\\
		0.1    & 0.5\\
		\hline
		\end{tabular}
		\caption{Table of tested the values of $\alpha$. Ranging from 0.01 to 0.5}
		\label{tab:test_alpha}
	\end{table}
\end{minipage}
\hfill
\begin{minipage}[c]{0.3\textwidth}
	\begin{table}[H]
	\centering
	\begin{tabular}{l r}
		\hline
		\multicolumn{2}{l}{\textbf{Distance punishments}}\\ 			\hline
		Start to room 3   & 0\\
		Room 1 to room 2  & -1.668\\
		Room 2 to room 3  & -2.592\\
		Room 3 to room 4  & -2.328\\
		Room 3 to room 5  & -2.544\\
		\hline
	\end{tabular}
	\caption{Table of the distance punishments. The distances are found in the test in appendix (\ref{subsec:est_path_length})}
	\label{tab:distance_punishment_5_rooms_1}
\end{table}
\end{minipage}

\clearpage
\subsubsection*{Data}
In figure (\ref{fig:q-learn_alpha_reward}) the average reward for all tests can be seen. It can be seen that the lower the value of $\alpha$ the higher average reward, meaning that the policy will converge to the optimal policy. Whereas a higher value of $\alpha$ will result in a lower average reward. It would also seem like the average reward are oscillating more.\par
In figure (\ref{fig:q-learn_alpha_iterations}) the average number of iterations per episoden can be seen. It can be seen that the higher the value of $\alpha$ the lower the average number of iterations will be. This is due to the fact, that the lower the learning rate are, the slower the algorithm learns, and will therefore need more steps to get to the goal.
\begin{figure}[H]
	\centering
	\begin{subfigure}[b]{0.49\textwidth}
		\centering
		\includegraphics[width=\textwidth]{MatlabPlots/q_learning_5_rooms_test_alpha_average_reward}
		\caption{Plot of how the average reward develops as a function of the number of episodes for each value of $\epsilon$}
		\label{fig:q-learn_alpha_reward}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.49\textwidth}
		\centering
		\includegraphics[width=\textwidth]{MatlabPlots/q_learning_5_rooms_test_alpha_average_iterations}
		\caption{Plot of how the average number of episodes develops as a function of the number of episodes for each value of $\alpha$}
		\label{fig:q-learn_alpha_iterations}
	\end{subfigure}
	\caption{Plots of both the average reward and average number of iterations pr. episode for each value of $\alpha$}
	\label{fig:q-learn_alpha}
\end{figure}
A value of 0.025 have been chosen as the best compromise between a high average reward and a low average number of iterations per episode.

\subsubsection*{Conclusion}
It can be concluded that the smaller the value of $\alpha$ the higher the average reward will be for a given number of episodes.\\
It can as well be concluded that the higher the value of $\alpha$ the lower the number of iterations per episode will be.\\ 
The value of 0.025 was chosen as the best compromise.


\end{document}