\documentclass[../Head/Main.tex]{subfiles}
\begin{document}
\subsection{The impact of $\epsilon$ on Q-learning performance}
The purpose of this test is to show how different values of $\epsilon$ influences the performance of Q-learning.
\subsubsection{Description of test}
This test was done by performing 100 trials of each selected value of $\epsilon$ for a range of episodes. The test was conducted using the world "5-room world" seen on figure \ref{fig:5_room_world}.\\
The distance punishments are bases on those found in the test in appendix \ref{subsec:est_path_length}. The distance punishments was scaled with a factor of 1.2 to make the distance a bigger factor in final path.\\
The probabilities used for each room was found in the test in appendix \ref{subsec:probability_test}. These probabilities was divided by the maximal value and scaled by a factor of 20. This was done to ensure that the total reward for entering a room the first time would be positive.\par 
The initial state for all tests was set to room 3 in order to ensure greatest number of possible paths.

\begin{figure}[H]
	\centering
		\begin{tikzpicture}
		\node[anchor=south west, inner sep=0] (image) at (0,0) {\includegraphics[width=0.65\textwidth]{map_5_rooms}};
		\node[align=center, black, font={\large}] at (1.25,3.25) {Room 1};
		\node[align=center, black, font={\large}] at (2,1.25) {Room 2};
		\node[align=center, black, font={\large}] at (5.45,2.25) {Room 3};
		\node[align=center, black, font={\large}] at (9,3) {Room 4};
		\node[align=center, black, font={\large}] at (9,0.9) {Room 5};
		\end{tikzpicture}		
	\caption{Illustration of "5-room world"}
	\label{fig:5_room_world}
\end{figure}

\subsubsection{Test parameters}
In the following tables, the parameters for the test can be seen. In order not to make the agent act completely randomly, it have been chosen to make the test on values of $\epsilon$ between 0.01 and 0.5.\\
\begin{minipage}[c]{0.35\textwidth}
	\begin{tabular}{l r}
	- World used                & 5-room world\\
	- Initial room              & room 3\\	
	- Probabilities based on    & 50 tests\\	
	- Number of tests           & 100\\
	- Scaling factor distance   & 1.2\\
	- Scaling factor reward     & 20\\
	- Learning rate $\alpha$    & 0.1\\
	- Discount factor $\gamma$  & 0.9\\
	\end{tabular}
\end{minipage}	
\hfill
\begin{minipage}[c]{0.2\textwidth}
	\begin{table}[H]
		\centering
		\begin{tabular}{r r}
		\hline
		\multicolumn{2}{l}{\textbf{Tested values of $\epsilon$}}\\ 			\hline
		0.01   & 0.15\\
		0.025  & 0.2\\
		0.05   & 0.3\\
		0.075  & 0.4\\
		0.1    & 0.5\\
		\hline
		\end{tabular}
		\caption{Table of tested the values of $\epsilon$. Ranging from 1 \% to 50 \%}
		\label{tab:test_epsilon}
	\end{table}
\end{minipage}
\hfill
\begin{minipage}[c]{0.3\textwidth}
	\begin{table}[H]
	\centering
	\begin{tabular}{l r}
		\hline
		\multicolumn{2}{l}{\textbf{Distance punishments}}\\ 			\hline
		Start to room 3   & 0\\
		Room 1 to room 2  & -1.668\\
		Room 2 to room 3  & -2.592\\
		Room 3 to room 4  & -2.328\\
		Room 3 to room 5  & -2.544\\
		\hline
	\end{tabular}
	\caption{Table of the distance punishments. The distances are found in the test in appendix \ref{subsec:est_path_length}}
	\label{tab:distance_punishment_5_rooms}
\end{table}
\end{minipage}

\clearpage
\subsubsection{Data}
On figure \ref{fig:q-learn_epsilon_reward} the average reward for all tests can be seen. It can be seen that the lower the value of $\epsilon$ the higher average reward, meaning that the policy will converge to the optimal policy.\par
On figure \ref{fig:q-learn_epsilon_iterations} the average number of iterations per episoden can be seen. It can be seen that the higher the value of $\epsilon$ the lower the average number of iterations will be. Given the fact that the more random the agent acts, the faster the agent will search alternative paths to the policy, and find its way through the environment.  
\begin{figure}[H]
	\centering
	\begin{subfigure}[b]{0.49\textwidth}
		\centering
		\includegraphics[width=\textwidth]{q_learning_5_rooms_test_epsilon_average_reward}
		\caption{Plot of how the average reward develops as a function of the number of episodes for each value of $\epsilon$}
		\label{fig:q-learn_epsilon_reward}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.49\textwidth}
		\centering
		\includegraphics[width=\textwidth]{q_learning_5_rooms_test_epsilon_average_iterations}
		\caption{Plot of how the average number of episodes develops as a function of the number of episodes for each value of $\epsilon$}
		\label{fig:q-learn_epsilon_iterations}
	\end{subfigure}
	\caption{Plots of both the average reward and average number of iterations pr. episode for each value of $\epsilon$}
	\label{fig:q-learn_epsilon}
\end{figure}
A value of 0.05 have been chosen as the best compromise between a high average reward and a low average number of iterations per episode.

\subsubsection{Conclusion}
It can be concluded that the smaller the value of $\epsilon$ the higher the average reward will be for a given number of episodes.\\
It can as well be concluded that the higher the value of $\epsilon$ the lower the number of iterations per episode will be.\\ 
The value of 0.05 was chosen as the best compromise.


\end{document}